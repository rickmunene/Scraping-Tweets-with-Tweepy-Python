{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Tweets with Tweepy Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is a step by step guide to scrape Twitter tweets using a Python library called Tweepy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prerequisites: Setting up a Twitter Developer Account**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start using Tweepy, you would need a Twitter Developer Account in order to call Twitter’s APIs. Just follow the instructions and after some time (only a few hours for me), they would grant you your access.\n",
    "\n",
    "To get the data required, we'll stream the tweets and replies from the official handles of the top banks, telcos, utilities services in Kenya using Tweepy and Twitter's API.\n",
    "\n",
    "The resulting dataset contains tweets and responses from Kenyan banks. The following data wrangling process will accomplish:\n",
    "- Get customer inquiry, and corresponding response from the company in every row.\n",
    "- Convert datetime columns to datetime data type.\n",
    "- Calculate response time to minutes.\n",
    "- Any customer inquiry that takes longer than 60 minutes will be filtered out. We are working on requests that get response in 60 minutes.\n",
    "- Create time attributes and response word count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the required libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import tweepy\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "colors = ['#D55E00', '#009E73', '#0072B2', '#348ABD', '#A60628', \n",
    "          '#7A68A6', '#467821', '#CC79A7', '#56B4E9', '#F0E442']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticating Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Authenticating Twitter APIpd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticating Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy really does make OAuth mostly painless. We'll need to get our credentials from Twitter Developer Account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that contains the credentials to access Twitter API\n",
    "ACCESS_TOKEN = ''\n",
    "ACCESS_SECRET = ''\n",
    "CONSUMER_KEY = ''\n",
    "CONSUMER_SECRET = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup access to API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)  # add proxy='172.30.1.251:6969' if needed\n",
    "    return api\n",
    "\n",
    "\n",
    "# Create API object\n",
    "api = connect_to_twitter_OAuth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now just to make sure it worked, lets print the most recent tweets from a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets from a specific user\n",
    "my_tweets = api.user_timeline(id='realDonaldTrump', count=10)\n",
    "\n",
    "for tweet in my_tweets:\n",
    "    print(tweet.created_at, tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use a function to extract the attributes we’re interested in and create a dataframe. Using a loop, we are able to extract tweets from multiple handles (banks in this case)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in tweepy.Cursor(api.user_timeline, screen_name='KCBGroup', count=3).pages(2):\n",
    "    for item in page:\n",
    "        #print('https://twitter.com/' + item.in_reply_to_screen_name + '/status/' + item.in_reply_to_status_id_str)\n",
    "        print(item.user.name)\n",
    "        print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limited number of API calls one can make using a basic and free developer account, (~900 calls every 15 minutes before your access is denied) we pass the ***wait_on_rate_limit=True, wait_on_rate_limit_notify=True*** paramentes during authentication which will automatically handle the limitation for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we scrape 20 tweets and loop over 50 pages per handle. Note: there are pagination limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = ['KCBGroup','AbsaKenya','coopbankenya','NCBABankKenya','KeEquityBank','StanChartKE','Safaricom_Care','AIRTEL_KE',\n",
    "           'Zuku_WeCare','KenyaPower_Care','DStv_Kenya','HudumaKenya']\n",
    "\n",
    "import io\n",
    "\n",
    "\n",
    "merged=pd.DataFrame()\n",
    "\n",
    "for handle in handles:\n",
    "    pages = tweepy.Cursor(api.user_timeline, screen_name=handle, count=20).pages(50)\n",
    "\n",
    "    for page in pages:\n",
    "        for tweet in page:\n",
    "            print(tweet.user.name, tweet.created_at, tweet.id, tweet.in_reply_to_status_id_str, \n",
    "                  tweet.in_reply_to_screen_name, tweet.text, tweet.retweet_count, tweet.favorite_count, \n",
    "                  sep='\\t', end='\\n', file=open(\"data/banks_tweeter_data_11262020.txt\", \"a\",  encoding='utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names=['User', 'Created_at', 'ID', 'Reply_to_status', 'Reply_to_user', 'Tweet', 'RTs','Likes'] \n",
    "df_t1 = pd.read_csv(r'data/banks_tweeter_data_11262020.txt', \n",
    "                    sep='\\t', names=col_names, header=None, quoting=3, error_bad_lines=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_t1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we are interested in getting the response time to a tweet, we'll write another funtion to get the attributes of the original tweet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ids = df_t1.Reply_to_status.tolist()\n",
    "\n",
    "#Collect all tweets & replies from the account (Original tweeet to companyh)\n",
    "def extract_tweet2_attributes(pages):\n",
    "    df_t2 = pd.DataFrame(columns=['O_ID', 'O_Created_at', 'O_User', 'O_Tweet'])\n",
    "\n",
    "    for _id in tweet_ids:\n",
    "        try:\n",
    "            tweet2 = api.get_status(_id)\n",
    "        except:\n",
    "            pass\n",
    "        #print(tweet2.id_str, tweet2.created_at, tweet2.user.screen_name, tweet2.text, reply_to_tweet2)\n",
    "\n",
    "        O_id = tweet2.id_str\n",
    "        O_time = tweet2.created_at\n",
    "        O_user = tweet2.user.screen_name\n",
    "        O_text = tweet2.text\n",
    "\n",
    "        new_row = {'O_ID': O_id, 'O_Created_at': O_time, 'O_User': O_user,'O_Tweet': O_text}\n",
    "\n",
    "        df_t2= df_t2.append(new_row, ignore_index=True, sort = False)\n",
    "\n",
    "    return(df_t2)\n",
    "\n",
    "df_t2 = extract_tweet2_attributes(pages)        \n",
    "#print(df_t2)\n",
    "\n",
    "#Export collected data to csv\n",
    "#df_t2.to_csv('safaricom_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We merge the two dataframes. The unified dataframe contains the time a user made a tweet to a bank and the time the bank responded to the user. This will help us extract the response time for each bank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_t1, df_t2, how='outer', left_on=['Reply_to_status'], right_on = ['O_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating time between outbound response and inbound message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Created_at\", \"O_Created_at\"]] = df[[\"Created_at\", \"O_Created_at\"]].apply(pd.to_datetime, errors = 'coerce', infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Response_time'] = df['Created_at'] - df['O_Created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to minutes, we only need tweets responded to in max 60 minutes\n",
    "df['Response_time'] = df['Response_time'].astype('timedelta64[s]') / 60\n",
    "df = df.loc[df['Response_time'] <= 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time attributes\n",
    "df['Created_at_dayofweek'] = df['Created_at'].apply(lambda x: x.dayofweek)\n",
    "df['Created_at_day_of_week'] = df['Created_at'].dt.day_name()\n",
    "df['Created_at_day'] = df['Created_at'].dt.day\n",
    "df['Created_at_is_weekend'] = df['Created_at_dayofweek'].isin([5,6]).apply(lambda x: 1 if x == True else 0)\n",
    "df['Tweet_word_count'] = df.O_Tweet.apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df[df.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup any duplicates from our loop\n",
    "df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export our data in as csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/banks_tweets_replies_11262020.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook can be located on the Github. Link on my blog thesiliconsavannah.com."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
